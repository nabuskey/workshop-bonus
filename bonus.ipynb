{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ca67c70-4ea1-4518-9983-225df079e535",
   "metadata": {},
   "source": [
    "# Bonus\n",
    "\n",
    "In the real world, debugging agents often need to be able to read logs from another system. Let's give the agent ability to read logs from a database instead. \n",
    "\n",
    "**Be sure to restart the kernel before proceeding**\n",
    "\n",
    "Let's create a job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1a3212-3ac1-45c2-9e5f-d8cd946de5b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /home/jovyan/bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db753fc9-4250-4868-88f8-a09fde7d160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip logs.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d2dfbc-b5c7-4062-bbb6-ce41243d652d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import boto3\n",
    "from datetime import timedelta\n",
    "from langchain_aws import ChatBedrock, ChatBedrockConverse\n",
    "from langchain_core.runnables import RunnableConfig\n",
    "from langchain_core.tools import BaseTool\n",
    "from langchain_mcp_adapters.sessions import StreamableHttpConnection\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from utils import print_agent_response, execute_query\n",
    "from phoenix.otel import register\n",
    "import os\n",
    "\n",
    "\n",
    "tracer_provider = register(\n",
    "  project_name=\"default\", # Default is 'default'\n",
    "  auto_instrument=True,\n",
    "  endpoint =  \"http://phoenix-0.phoenix.phoenix.svc.cluster.local:6006/v1/traces\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bef231a-df49-4258-a32e-ccfdcc5479c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit the number of times an agent runs through LLM to 10\n",
    "recursion_limit = 30\n",
    "model = \"us.anthropic.claude-3-7-sonnet-20250219-v1:0\"\n",
    "# We are using Claude hosted on Bedrock, so let's create a Bedrock session\n",
    "bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "# Use the session to define a LLM that will drive the Agent\n",
    "llm = ChatBedrockConverse(\n",
    "    model=model,\n",
    "    temperature=0,\n",
    "    max_tokens=None,\n",
    "    client=bedrock_client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b5ae0-051f-4233-aa55-a625e3dbbbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a system prompt. Let the LLM know what its role is and guidelines\n",
    "system_prompt=\"\"\"\n",
    "You are Sparky McSparkface, a specialized Apache Spark troubleshooting assistant designed for data engineering teams.\n",
    "Your purpose is to diagnose, analyze, and resolve Spark application issues by systematically examining logs, metrics, and application data.\n",
    "\n",
    "Core Capabilities:\n",
    "- Troubleshoot Spark job failures and identify root causes\n",
    "- Recommend performance optimizations for Spark applications\n",
    "- Analyze resource utilization patterns and bottlenecks\n",
    "- Provide technical, evidence-based explanations tailored to various expertise levels\n",
    "\n",
    "Output Format:\n",
    "Structure your responses as:\n",
    "\n",
    "## Summary\n",
    "Brief overview of the primary issue and severity level (Critical/High/Medium/Low)\n",
    "\n",
    "## Findings\n",
    "For each issue identified:\n",
    "- **Issue**: Clear description\n",
    "- **Evidence**: Specific log excerpts, metrics, or configuration values\n",
    "- **Impact**: How this affects performance/functionality\n",
    "- **Recommendation**: Actionable solution with configuration changes\n",
    "\n",
    "## Next Steps\n",
    "Prioritized list of actions to take\n",
    "\n",
    "Evidence Requirements:\n",
    "Only include findings supported by concrete evidence such as log excerpts with timestamps, specific metric values, configuration settings, or resource utilization data.\n",
    "\n",
    "If you cannot find supporting evidence for a potential issue, exclude it from your response.\n",
    "If you lack sufficient information for confident assessment, state: \"I don't have enough information to confidently assess this.\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "agent = create_react_agent(\n",
    "    model=llm,\n",
    "    tools=[execute_query],\n",
    "    prompt=SystemMessage(system_prompt)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7326e8-6bfa-4906-b1a7-8c1272cb79a3",
   "metadata": {},
   "source": [
    "# Ready?\n",
    "\n",
    "Let's see if it can tell us why the Spark application failed. It has access to the logs, it can tell us what's wrong with our app right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c4b8d7-30c6-4647-9b68-d833e44093a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await agent.ainvoke(\n",
    "    input={\"messages\": [HumanMessage(content=\"hi, tell me why spark-2435f7978c3146baacc286c789bed535 failed?\")]},\n",
    "    config=RunnableConfig(recursion_limit=recursion_limit),\n",
    ")\n",
    "\n",
    "print_agent_response(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a9e588-9c9b-49c6-993b-ff80d8200340",
   "metadata": {},
   "source": [
    "# Uh oh\n",
    "\n",
    "You likely gotten a big stack of errors after waiting a minute. What happened?\n",
    "\n",
    "Head over to your tracing endpoint and see if you can tell what's going on.\n",
    "\n",
    "\n",
    "## What likely happened\n",
    "\n",
    "LLMs are quite good at creating SQL queries. However, this LLM was not given enough information about the table such as the schema. So it likely went through the trial and error process to find the right schema, right query, and right context amount.\n",
    "\n",
    "\n",
    "## How can you fix it?\n",
    "\n",
    "Here's the schema for the table. Can you think of a way to fix this?\n",
    "\n",
    "\n",
    "```\n",
    "• id (INTEGER, PRIMARY KEY, AUTOINCREMENT) - Unique identifier for each log entry\n",
    "• spark_app_id (TEXT) - Application identifier from the JSON data (e.g., \"app-12345\")\n",
    "• log_message (TEXT) - The actual log message content, including multi-line stack traces\n",
    "• log_level (TEXT) - Log severity level (e.g., \"ERROR\", \"INFO\", \"WARN\")\n",
    "• time (TIMESTAMP) - Parsed timestamp from log entries in format YY/MM/DD HH:MM:SS\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6963d9eb-e72a-4acf-863b-c4c09b56c423",
   "metadata": {},
   "source": [
    "# Potential fixes\n",
    "\n",
    "<details>\n",
    "<summary><strong>Solution 1</strong></summary>\n",
    "\n",
    "Provide the schmea information in the system promopt or the function comment. For example:\n",
    "\n",
    "```\n",
    "you have access to a SQLite table that contains log messages from failed applications. The schema of the table is as follows:\n",
    "• id (INTEGER, PRIMARY KEY, AUTOINCREMENT) - Unique identifier for each log entry\n",
    "• spark_app_id (TEXT) - Application identifier from the JSON data (e.g., \"app-12345\")\n",
    "• log_message (TEXT) - The actual log message content, including multi-line stack traces\n",
    "• log_level (TEXT) - Log severity level (e.g., \"ERROR\", \"INFO\", \"WARN\")\n",
    "• time (TIMESTAMP) - Parsed timestamp from log entries in format YY/MM/DD HH:MM:SS\n",
    "\n",
    "Because the number of rows returned by a query could be large, be sure to limit your query to 10 - 20 and select the log_message column only.\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary><strong>Solution 2</strong></summary>\n",
    "\n",
    "Create a dedicated tool that retrieves data from the table without the LLM generating SQL queries. For example:\n",
    "\n",
    "```\n",
    "@tool\n",
    "def get_logs_by_app_id(spark_app_id, limit=10):\n",
    "   \"\"\"\n",
    "   Get logs for a specific Spark application ID.\n",
    "\n",
    "   Args:\n",
    "       spark_app_id (str): The Spark application ID to filter by\n",
    "       limit (int): Maximum number of rows to return (default: 10)\n",
    "\n",
    "   Returns:\n",
    "       list: List of tuples containing (id, spark_app_id, log_message, log_level, time)\n",
    "   \"\"\"\n",
    "   cursor = conn.cursor()\n",
    "   cursor.execute(\n",
    "       \"SELECT  FROM logs WHERE sparkapp_id = ? ORDER BY time DESC LIMIT ?\",\n",
    "       (spark_app_id, limit)\n",
    "   )\n",
    "   return cursor.fetchall()\n",
    "```\n",
    "</details>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
